{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "102003615.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Text Summarizer and Question Answering System Using NLTK by Anmol Parmar(102003615)**"
      ],
      "metadata": {
        "id": "tBjQ6Wsca4Gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Text summarization is the process of shortening long pieces of text while preserving key information content and overall meaning, to create a subset (a summary) that represents the most important or relevant information within the Text.*"
      ],
      "metadata": {
        "id": "TmWBeDWibL_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "@Importing the libraries:"
      ],
      "metadata": {
        "id": "YVlGvIL4bZ3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLEASE INPUT THE FILES GIVEN AS THE DATASETS WHICH ARE 1.TXT,2.TXT---------140.TXT"
      ],
      "metadata": {
        "id": "8_R8XN6_oAkZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vDU0vrI6OV7x"
      },
      "outputs": [],
      "source": [
        "#importing the natural learning toolkit and importing the stopwords which we have to remove>\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import csv\n",
        "#for proper nouns\n",
        "from nltk.tag import pos_tag \n",
        "#for tokenizing the words \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#combining all the data in one file and then creating the summary from it.\n",
        "filenames = [\"1.txt\",\"2.txt\",\"3.txt\",\"4.txt\",\"5.txt\",\"6.txt\",\"7.txt\",\"8.txt\",\"9.txt\",\"10.txt\",\"11.txt\",\"12.txt\",\"13.txt\",\"14.txt\",\"15.txt\",\"16.txt\",\"17.txt\",\"18.txt\",\"19.txt\",\"20.txt\",\"21.txt\",\"22.txt\",\"23.txt\",\"24.txt\",\"25.txt\",\"26.txt\",\"27.txt\",\"28.txt\",\"29.txt\",\"30.txt\",\"31.txt\",\"32.txt\",\"33.txt\",\"34.txt\",\"35.txt\",\"36.txt\",\"37.txt\",\"38.txt\",\"39.txt\",\"40.txt\",\"41.txt\",\"42.txt\",\"43.txt\",\"44.txt\",\"45.txt\",\"46.txt\",\"47.txt\",\"48.txt\",\"49.txt\",\"50.txt\",\"51.txt\",\"52.txt\",\"53.txt\",\"54.txt\",\"55.txt\",\"56.txt\",\"57.txt\",\"58.txt\",\"59.txt\",\"60.txt\",\"61.txt\",\"62.txt\",\"63.txt\",\"64.txt\",\"65.txt\",\"66.txt\",\"68.txt\",\"69.txt\",\"70.txt\",\"71.txt\",\"72.txt\",\"73.txt\",\"74.txt\",\"75.txt\",\"76.txt\",\"77.txt\",\"78.txt\",\"79.txt\",\"80.txt\",\"81.txt\",\"82.txt\",\"83.txt\",\"84.txt\",\"85.txt\",\"86.txt\",\"87.txt\",\"88.txt\",\"89.txt\",\"90.txt\",\"91.txt\",\"92.txt\",\"93.txt\",\"94.txt\",\"95.txt\",\"96.txt\",\"97.txt\",\"98.txt\",\"99.txt\",\"100.txt\",\"101.txt\",\"102.txt\",\"103.txt\",\"104.txt\",\"105.txt\",\"106.txt\",\"107.txt\",\"108.txt\",\"109.txt\",\"110.txt\",\"111.txt\",\"112.txt\",\"113.txt\",\"114.txt\",\"115.txt\",\"116.txt\",\"117.txt\",\"118.txt\",\"119.txt\",\"120.txt\",\"121.txt\",\"122.txt\",\"123.txt\",\"124.txt\",\"125.txt\",\"126.txt\",\"127.txt\",\"128.txt\",\"129.txt\",\"130.txt\",\"131.txt\",\"132.txt\",\"133.txt\",\"134.txt\",\"135.txt\",\"136.txt\",\"137.txt\",\"138.txt\",\"139.txt\"]\n",
        "\n",
        "\n",
        "with open(\"140.txt\", \"w\") as outfile:\n",
        "\n",
        "    for filename in filenames:\n",
        "\n",
        "        with open(filename,encoding=\"utf8\", errors='ignore') as infile:\n",
        "\n",
        "            contents = infile.read()\n",
        "\n",
        "            outfile.write(contents)"
      ],
      "metadata": {
        "id": "WiKsKUe6e6dl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "b7a25787-ae2c-48a1-a5b8-f21bc350141f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-698541ad46e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for opening the file that is combined by the above code \n",
        "nltk.download('punkt')\n",
        "filename=\"140.txt\"\n",
        "f = open((filename), \"r\")\n",
        "text=f.read()\n",
        "f.close()\n",
        "print(text)\n",
        "#converting whole text into tokens\n",
        "sentences = sent_tokenize(text) # NLTK function\n",
        "total_documents = len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PndSgItVO0yW",
        "outputId": "df74eb9b-23a8-40ba-aab7-f003db5d4905"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature-1 (tf-idf matrix)**\n"
      ],
      "metadata": {
        "id": "rz4Q_GwchNCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the stopwords\n",
        "nltk.download('stopwords')\n",
        "import math\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()  \n",
        "stop_words = set(stopwords.words('english'))\n",
        "clean_words = []\n",
        "for sent in sentences:\n",
        "        words = word_tokenize(sent)\n",
        "        words = [ps.stem(word.lower()) for word in words if word.isalnum()]\n",
        "        clean_words += [word for word in words if word not in stop_words]\n",
        "tf_matrix = {}\n",
        "for sentence in sentences:\n",
        "        tf_table = {}        \n",
        "        words_count = len(sentence)       \n",
        "        word_freq = {}\n",
        "        for word in clean_words:\n",
        "            word_freq[word] = (word_freq[word] + 1) if word in  word_freq else 1 \n",
        "        for word, count in word_freq.items():\n",
        "            tf_table[word] = count / words_count        \n",
        "            tf_matrix[sentence[:15]] = tf_table  \n",
        "idf_matrix = {}\n",
        "\n",
        "documents_count = len(sentences)\n",
        "sentence_word_table = {}\n",
        "\n",
        "    # Getting words in the sentence\n",
        "for sentence in sentences:\n",
        "        clean_words = text_preprocessing([sentence])\n",
        "        sentence_word_table[sentence[:15]] = clean_words\n",
        "\n",
        "    # Determining word count table with the count of sentences which contains the word.\n",
        "word_in_docs = {}\n",
        "for sent, words in sentence_word_table.items():\n",
        "        for word in words:\n",
        "            word_in_docs[word] = (word_in_docs[word] + 1) if word in word_in_docs else 1\n",
        "\n",
        "    # Determining idf of the words in the sentence.\n",
        "for sent, words in sentence_word_table.items():\n",
        "        idf_table = {}\n",
        "        for word in words:\n",
        "            idf_table[word] = math.log10(documents_count / float(word_in_docs[word]))\n",
        "\n",
        "        idf_matrix[sent] = idf_table\n",
        "tf_idf_matrix = {}\n",
        "\n",
        "for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
        "        tf_idf_table = {}\n",
        "\n",
        "        for (word1, value1), (word2, value2) in zip(f_table1.items(), f_table2.items()):\n",
        "            tf_idf_table[word1] = float(value1 * value2)\n",
        "\n",
        "        tf_idf_matrix[sent1] = tf_idf_table\n",
        "sentence_value = {}\n",
        "\n",
        "for sent, f_table in tf_idf_matrix.items():\n",
        "        total_score_per_sentence = 0\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, score in f_table.items():\n",
        "            total_score_per_sentence += score\n",
        "\n",
        "        sentence_value[sent] = total_score_per_sentence / count_words_in_sentence\n",
        "print(sentence_value.values())        "
      ],
      "metadata": {
        "id": "CWyEtrJgPB_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d797e752-d835-4db4-f634-40370acb0b0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "dict_values([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature-2(Q OPhrases)**"
      ],
      "metadata": {
        "id": "r0sVC7juhesD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for cue phrases\n",
        "# ......................feature 2 (cue phrases).................\n",
        "QPhrases=[\"incidentally\", \"example\", \"anyway\", \"furthermore\",\"according\"\n",
        "            \"first\", \"second\", \"then\", \"now\", \"thus\", \"moreover\", \"therefore\", \"hence\", \"lastly\", \"finally\", \"summary\",\"additionally\",\"here\",\"on the other hand\",\"by the way\"]\n",
        "\n",
        "cue_phrases={}\n",
        "for sentence in sentences:\n",
        "    cue_phrases[sentence] = 0\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    for word in word_tokens:\n",
        "        if word.lower() in QPhrases:\n",
        "            cue_phrases[sentence] += 1\n",
        "maximum_frequency = max(cue_phrases.values())\n",
        "for k in cue_phrases.keys():\n",
        "    try:\n",
        "        cue_phrases[k] = cue_phrases[k] / maximum_frequency\n",
        "        cue_phrases[k]=round(cue_phrases[k],3)\n",
        "    except ZeroDivisionError:\n",
        "        x=0\n",
        "print(cue_phrases.values())"
      ],
      "metadata": {
        "id": "_eKqPb1nQrLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature-3(Numerical Data)**"
      ],
      "metadata": {
        "id": "9mMy7M0Vig34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# .......................feature 3 (numerical data)...................\n",
        "numeric_data={}\n",
        "for sentence in sentences:\n",
        "    numeric_data[sentence] = 0\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    for word in word_tokens:\n",
        "        if word.isdigit():\n",
        "            numeric_data[sentence] += 1\n",
        "maximum_frequency = max(numeric_data.values())\n",
        "for k in numeric_data.keys():\n",
        "    try:\n",
        "        numeric_data[k] = (numeric_data[k]/maximum_frequency)\n",
        "        numeric_data[k] = round(numeric_data[k], 3)\n",
        "    except ZeroDivisionError:\n",
        "        x=0\n",
        "print(numeric_data.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGVdANryitU6",
        "outputId": "640a5ee4-8059-480b-c0e3-65b854e3b3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_values([0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature-4(sentence length)**"
      ],
      "metadata": {
        "id": "MzGdm1Bhi1Q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#....................feature -4 (sentence length)........................\n",
        "sent_len_score={}\n",
        "for sentence in sentences:\n",
        "    sent_len_score[sentence] = 0\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    if len(word_tokens) in range(0,10):\n",
        "        sent_len_score[sentence]=1-0.05*(10-len(word_tokens))\n",
        "    elif len(word_tokens) in range(7,20):\n",
        "        sent_len_score[sentence]=1\n",
        "    else:\n",
        "        sent_len_score[sentence]=1-(0.05)*(len(word_tokens)-20)\n",
        "for k in sent_len_score.keys():\n",
        "    sent_len_score[k]=round(sent_len_score[k],4)\n",
        "print(sent_len_score.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTQDclkti9Yb",
        "outputId": "3ca78432-a76e-4267-bd42-3acb61ba151e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_values([1.0, 0.85, -0.1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature-5(sentence position)**"
      ],
      "metadata": {
        "id": "rpTJFBIajT1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#....................feature-5(sentence position)........................\n",
        "sentence_position={}\n",
        "d=1\n",
        "no_of_sent=len(sentences)\n",
        "for i in range(no_of_sent):\n",
        "    a=1/d\n",
        "    b=1/(no_of_sent-d+1)\n",
        "    sentence_position[sentences[d-1]]=max(a,b)\n",
        "    d=d+1\n",
        "for k in sentence_position.keys():\n",
        "    sentence_position[k]=round(sentence_position[k],3)\n",
        "print(sentence_position.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXGk8-_CjepD",
        "outputId": "b5fb60e0-abdc-4352-edbb-c8d0c5e4708c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_values([1.0, 0.5, 1.0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature-6(upper case)**"
      ],
      "metadata": {
        "id": "wiPIhkKujsvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#........................feature-6 (upper cases).................................\n",
        "upper_case={}\n",
        "for sentence in sentences:\n",
        "    upper_case[sentence] = 0\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    for k in word_tokens:\n",
        "        if k.isupper():\n",
        "            upper_case[sentence] += 1\n",
        "maximum_frequency = max(upper_case.values())\n",
        "for k in upper_case.keys():\n",
        "    try:\n",
        "        upper_case[k] = (upper_case[k]/maximum_frequency)\n",
        "        upper_case[k] = round(upper_case[k], 3)\n",
        "    except ZeroDivisionError:\n",
        "        x=0\n",
        "print(upper_case.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN8xdZvskFUx",
        "outputId": "ae368e06-ec4f-4ed6-89e6-efe58d3574a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_values([0.5, 0.0, 1.0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature-7(Number of Proper Noun)**"
      ],
      "metadata": {
        "id": "JnSyUSM-kRCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#......................... feature-7 (number of proper noun)...................\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "proper_noun={}\n",
        "for sentence in sentences:\n",
        "    tagged_sent = pos_tag(sentence.split())\n",
        "    propernouns = [word for word, pos in tagged_sent if pos == 'NNP']\n",
        "    proper_noun[sentence]=len(propernouns)\n",
        "maximum_frequency = max(proper_noun.values())\n",
        "for k in proper_noun.keys():\n",
        "    try:\n",
        "        proper_noun[k] = (proper_noun[k]/maximum_frequency)\n",
        "        proper_noun[k] = round(proper_noun[k], 3)\n",
        "    except ZeroDivisionError:\n",
        "        x=0\n",
        "print(proper_noun.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHEzzXwSkZgW",
        "outputId": "41baecfb-e921-405f-d67f-fcd8b057262e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "dict_values([0.125, 0.0, 1.0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature-8(Heading Match)**"
      ],
      "metadata": {
        "id": "_QFKJGFUV6Tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heading = text.split('\\n')\n",
        "heading_token = nltk.word_tokenize(heading[0])"
      ],
      "metadata": {
        "id": "CtyJlsCSO3W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heading_match={}\n",
        "for sentence in sentences:\n",
        "  heading_match[sentence]=0\n",
        "  word_tokens = nltk.word_tokenize(sentence)\n",
        "  for word in word_tokens:\n",
        "      if word in heading_token:\n",
        "        heading_match[sentence]+=1\n",
        "maximum_frequency = max(heading_match.values())\n",
        "for k in heading_match.keys():\n",
        "    try:\n",
        "        heading_match[k] = (heading_match[k]/maximum_frequency)\n",
        "        heading_match[k] = round(heading_match[k], 3)\n",
        "    except ZeroDivisionError:\n",
        "        x=0\n",
        "print(heading_match.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWB5Re-uQQvj",
        "outputId": "39aa3208-0328-4435-ac80-9c8e08529160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_values([0.839, 0.226, 0.097, 0.161, 0.097, 0.097, 0.194, 0.065, 0.839, 0.226, 0.839, 0.226, 0.194, 0.226, 0.29, 0.258, 0.032, 0.032, 0.065, 0.065, 0.323, 0.581, 0.194, 0.065, 0.097, 0.097, 0.032, 0.065, 0.097, 0.129, 0.065, 0.097, 0.097, 0.226, 0.032, 0.097, 0.258, 0.484, 0.129, 0.29, 0.258, 0.097, 0.032, 0.097, 0.032, 0.613, 0.226, 0.161, 0.194, 0.194, 0.387, 0.0, 0.097, 0.323, 0.0, 0.065, 0.323, 0.032, 0.226, 0.032, 0.097, 0.29, 0.516, 0.161, 0.161, 0.194, 0.032, 0.29, 0.097, 0.097, 0.323, 0.194, 0.097, 0.355, 0.032, 0.161, 0.032, 0.226, 0.065, 0.129, 0.323, 0.065, 0.258, 0.129, 0.161, 0.097, 0.097, 0.032, 0.452, 0.71, 0.065, 0.065, 0.129, 0.097, 0.032, 0.194, 0.29, 0.161, 0.129, 0.065, 0.0, 0.355, 0.129, 0.129, 0.097, 0.258, 0.129, 0.226, 0.129, 0.032, 0.129, 0.129, 0.032, 0.194, 0.258, 0.032, 0.097, 0.194, 0.419, 0.194, 0.032, 0.065, 0.355, 0.065, 0.129, 0.452, 0.0, 0.065, 0.161, 0.129, 0.355, 0.226, 0.516, 0.161, 0.258, 0.29, 0.194, 0.71, 0.226, 0.065, 0.258, 0.097, 0.097, 0.323, 0.161, 0.387, 0.032, 0.097, 0.129, 0.065, 0.065, 0.0, 0.516, 0.0, 0.194, 0.065, 0.387, 0.548, 0.323, 0.065, 0.452, 0.226, 0.258, 0.097, 0.129, 0.29, 0.129, 0.097, 0.097, 0.129, 0.355, 0.258, 0.806, 0.387, 0.129, 0.161, 0.065, 0.194, 0.29, 0.161, 0.065, 0.129, 0.065, 0.065, 0.129, 0.0, 0.0, 0.29, 0.258, 0.129, 0.0, 0.129, 0.032, 0.129, 0.097, 0.065, 0.065, 0.032, 0.194, 0.032, 0.161, 0.0, 0.065, 0.097, 0.065, 0.032, 0.032, 0.065, 0.032, 0.032, 0.0, 0.065, 0.065, 0.161, 0.194, 0.065, 0.032, 0.032, 0.097, 0.129, 0.032, 0.065, 0.065, 0.032, 0.065, 0.097, 0.097, 0.065, 0.032, 0.097, 0.065, 0.097, 0.065, 0.097, 0.097, 0.065, 0.129, 0.097, 0.129, 0.097, 0.065, 0.097, 0.065, 0.0, 0.129, 0.194, 0.065, 0.0, 0.065, 0.0, 0.0, 0.129, 0.097, 0.226, 0.065, 0.129, 0.0, 0.0, 0.032, 0.129, 0.129, 0.032, 0.097, 0.065, 0.161, 0.161, 0.065, 0.129, 0.129, 0.032, 0.032, 0.097, 0.032, 0.065, 0.065, 0.258, 0.161, 0.065, 0.032, 0.226, 0.032, 0.226, 0.065, 0.194, 0.097, 0.258, 0.065, 0.065, 0.032, 0.0, 0.161, 0.129, 0.065, 0.29, 0.065, 0.032, 0.0, 0.0, 0.0, 0.0, 0.129, 0.032, 0.323, 0.161, 0.258, 0.161, 0.0, 0.032, 0.129, 0.097, 0.032, 0.129, 0.097, 0.129, 0.194, 0.194, 0.032, 0.194, 0.161, 0.032, 0.0, 0.065, 0.097, 0.032, 0.032, 0.161, 0.032, 0.129, 0.29, 0.161, 0.129, 0.065, 0.097, 0.065, 0.0, 0.194, 0.226, 0.419, 0.032, 0.097, 0.29, 0.226, 0.129, 0.065, 0.065, 0.387, 0.194, 0.065, 0.258, 0.097, 0.097, 0.226, 0.065, 0.065, 0.032, 0.258, 0.097, 0.032, 0.226, 0.129, 0.097, 0.0, 0.032, 0.194, 0.097, 0.355, 0.129, 0.194, 0.258, 0.032, 0.452, 0.0, 0.194, 0.097, 0.29, 0.032, 0.161, 0.226, 0.226, 0.194, 0.129, 0.258, 0.032, 0.032, 0.032, 0.355, 0.097, 0.452, 0.097, 0.097, 0.258, 0.161, 0.032, 0.065, 0.194, 0.032, 0.194, 0.065, 0.097, 0.032, 0.065, 0.226, 0.129, 0.323, 0.129, 0.129, 0.129, 0.129, 0.032, 0.194, 0.0, 0.065, 0.29, 0.355, 0.065, 0.484, 0.194, 0.129, 0.387, 0.032, 0.258, 0.129, 0.032, 0.097, 0.129, 0.129, 0.032, 0.097, 0.161, 0.065, 0.065, 0.097, 0.0, 0.129, 0.161, 0.065, 0.0, 0.194, 0.194, 0.032, 0.129, 0.129, 0.097, 0.0, 0.0, 0.161, 0.29, 0.032, 0.645, 0.032, 0.226, 0.097, 0.032, 0.0, 0.226, 0.129, 0.065, 0.032, 0.097, 0.29, 0.065, 0.097, 0.129, 0.323, 0.323, 0.097, 0.097, 0.129, 0.194, 0.419, 0.097, 0.29, 0.0, 0.258, 0.387, 0.129, 0.065, 0.161, 0.097, 0.097, 0.161, 0.161, 0.194, 0.355, 0.194, 0.129, 0.194, 0.065, 0.419, 0.0, 0.065, 0.29, 0.097, 0.065, 0.129, 0.226, 0.258, 0.387, 0.452, 0.129, 0.0, 0.097, 0.129, 0.032, 0.129, 0.161, 0.032, 0.129, 0.29, 0.0, 0.452, 0.129, 0.452, 0.097, 0.065, 0.258, 0.065, 0.097, 0.419, 0.032, 0.097, 0.161, 0.161, 0.0, 0.032, 0.065, 0.323, 0.065, 0.032, 0.097, 0.065, 0.032, 0.226, 0.129, 0.032, 0.065, 0.387, 0.194, 0.032, 0.129, 0.258, 0.129, 0.0, 0.097, 0.065, 0.097, 0.097, 0.065, 0.065, 1.0, 0.194, 0.516, 0.161, 0.097, 0.258, 0.129, 0.258, 0.065, 0.032, 0.226, 0.0, 0.452, 0.161, 0.097, 0.065])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for calculating the total scores\n",
        "total_score={}\n",
        "for k in cue_phrases.keys():\n",
        "    total_score[k]=cue_phrases[k]+numeric_data[k]+sent_len_score[k]+sentence_position[k]+upper_case[k]+proper_noun[k]\n",
        "print(total_score.values())  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "MCX7IaamtmTS",
        "outputId": "b81f2fe7-4546-48f3-86f9-fe89cbd0530a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-64434f3d0d3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtotal_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcue_phrases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcue_phrases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnumeric_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msent_len_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msentence_position\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mupper_case\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mproper_noun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cue_phrases' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in total_score:\n",
        "    if key in sentence_value:\n",
        "        total_score[key] = total_score[key] + sentence_value[key]"
      ],
      "metadata": {
        "id": "tBxGgZU9uyZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in total_score:\n",
        "    if key in heading_match:\n",
        "        total_score[key] = total_score[key] + heading_match[key]"
      ],
      "metadata": {
        "id": "yC9OKgE1dnqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sumValues = 0\n",
        "for sentence in total_score: \n",
        "    sumValues += total_score[sentence] \n",
        "average = int(sumValues / len(total_score)) \n",
        "   \n",
        "# Storing sentences into our summary. \n",
        "summary = '' \n",
        "for sentence in sentences: \n",
        "    if (sentence in total_score) and (total_score[sentence] > (1.3*average)): \n",
        "        summary += \" \" + sentence \n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_w5oNJIl7Qq",
        "outputId": "8f43ceb0-9f58-487e-8b15-7ef51fd029eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " COVID-19 and the world of work\n",
            "\n",
            "The world of work has been profoundly affected by the global virus pandemic. Virtually gathered at the 109th\n",
            "International Labour Conference, ILO's constituents – Governments, workers and employers – have adopted a Global Call to action for\n",
            "a human-centred recovery from the COVID-19 crisis that is inclusive, sustainable and resilient.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature-9(Named Entity Recognition)**"
      ],
      "metadata": {
        "id": "ZcBV5fEsXcG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Named Entity Recognition\n",
        "! pip install spacy\n",
        "! pip install nltk\n",
        "! python -m spacy download en_core_web_sm\n",
        "  \n",
        "# imports and load spacy english language package\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy import tokenizer\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp(summary)\n",
        "\n",
        "sentences = list(doc.sents)\n",
        "print(sentences)\n",
        "# tokenization\n",
        "for token in doc:\n",
        "    print(token.text)\n",
        "# print entities\n",
        "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
        "print(ents)\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "metadata": {
        "id": "1gv3rkPE1-26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question Answering System(Based on the summary given by Text Summarizer)**"
      ],
      "metadata": {
        "id": "Nk9lu5l12m0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts=summary"
      ],
      "metadata": {
        "id": "sf6H0TNk3P9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing stopwords from the summary\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "  \n",
        "text_tokenss= word_tokenize(texts)\n",
        "\n",
        "tokens_without_sws = [word for word in text_tokenss if not word in stopwords.words()]\n",
        "\n",
        "print(tokens_without_sws)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4AYyTW92xaw",
        "outputId": "5a804130-7a8d-401c-e00a-e4218cb53b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['COVID-19', 'world', 'work', 'The', 'world', 'work', 'profoundly', 'affected', 'global', 'virus', 'pandemic', '.', 'Virtually', 'gathered', '109th', 'International', 'Labour', 'Conference', ',', 'ILO', \"'s\", 'constituents', '–', 'Governments', ',', 'workers', 'employers', '–', 'adopted', 'Global', 'Call', 'action', 'human-centred', 'recovery', 'COVID-19', 'crisis', 'inclusive', ',', 'sustainable', 'resilient', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#joining the words\n",
        "total_summary = (\" \").join(tokens_without_sws)\n",
        "print(total_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqF3sVMs36vt",
        "outputId": "f283c24b-dc3d-4f63-ab6a-bd9de6636ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COVID-19 world work The world work profoundly affected global virus pandemic . Virtually gathered 109th International Labour Conference , ILO 's constituents – Governments , workers employers – adopted Global Call action human-centred recovery COVID-19 crisis inclusive , sustainable resilient .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#inputing the question and removing stopwords from it \n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = input(\"Enter your question: \")  \n",
        "text_tokens = word_tokenize(text)\n",
        "\n",
        "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "\n",
        "print(tokens_without_sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bwuamp227NH",
        "outputId": "9c9738a8-1377-4cee-cfdc-0a1908e7f637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Enter your question: what is covid-19\n",
            "['covid-19']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#joining the words\n",
        "filtered_sentence = (\" \").join(tokens_without_sw)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y01XQpW3Dbg",
        "outputId": "d8452f35-ba6f-4c82-eb1f-ecd917b232e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "covid-19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#searching the question\n",
        "!pip install Transformers\n",
        "from transformers import pipeline\n",
        "#text=input(\"enter the question\")\n",
        "qNa= pipeline(\"question-answering\")\n",
        "\n",
        "ans = qNa({'question': filtered_sentence,\n",
        "           'context': f'{total_summary}'})\n",
        "texting=ans['answer']\n",
        "print(ans)\n",
        "print(ans['answer'])"
      ],
      "metadata": {
        "id": "md9ZqPH23GBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Named Entity Recognition using Spacy\n",
        "! pip install spacy\n",
        "! pip install nltk\n",
        "! python -m spacy download en_core_web_sm\n",
        "  \n",
        "# imports and load spacy english language package\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy import tokenizer\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        " \n",
        "doc = nlp(texting)\n",
        "#doc2 = nlp(text2)\n",
        "sentences = list(doc.sents)\n",
        "print(sentences)\n",
        "# tokenization\n",
        "for token in doc:\n",
        "    print(token.text)\n",
        "# print entities\n",
        "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
        "print(ents)\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "metadata": {
        "id": "ZVw6m2xI3Lc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THANKS**"
      ],
      "metadata": {
        "id": "LcZpVZmCn6CD"
      }
    }
  ]
}